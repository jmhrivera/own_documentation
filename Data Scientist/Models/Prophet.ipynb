{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "\n",
    "- n_estimators = This parameter specifies the `number of decision trees in the forest`. It controls the complexity of the model. Higher values can lead to better performance but also increase computation time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Regressor\n",
    "| Parameter             | Accepted Values                     | Meaning                                                     | Common Values          |\n",
    "|-----------------------|-------------------------------------|-------------------------------------------------------------|------------------------|\n",
    "| objective           | String                              | Specifies the learning task and the corresponding loss function. | 'reg:squarederror' (MSE), 'reg:linear' (linear regression), 'binary:logistic' (logistic regression) |\n",
    "| learning_rate       | Float > 0                           | Controls the step size at each iteration of the boosting process. | 0.1, 0.01, 0.001      |\n",
    "| n_estimators        | Integer > 0                         | Number of boosting stages (trees) to be used in the ensemble. | 50, 100, ...          |\n",
    "| max_depth           | Integer >= 0 or None                | Maximum depth of the decision trees.                         | 3, 6, 10, None        |\n",
    "| min_child_weight    | Float >= 0                          | Minimum sum of instance weight (hessian) needed in a child.  | 1.0, 5.0, 10.0        |\n",
    "| subsample           | Float between 0 and 1               | Fraction of samples used for fitting the individual trees.   | 0.8, 0.9, 1.0         |\n",
    "| colsample_bytree    | Float between 0 and 1               | Fraction of features used for fitting the individual trees.  | 0.8, 0.9, 1.0         |\n",
    "| gamma               | Float >= 0                          | Minimum loss reduction required to make a further partition on a leaf node of the tree. | 0.0, 0.1, 0.2         |\n",
    "| reg_alpha           | Float >= 0                          | L1 regularization term on weights.                           | 0.0, 0.1, 0.5         |\n",
    "| reg_lambda          | Float >= 0                          | L2 regularization term on weights.                           | 0.0, 0.1, 1.0         |\n",
    "| scale_pos_weight    | Float >= 0                          | Control the balance of positive and negative weights.        | 1.0, 5.0, 10.0        |\n",
    "| max_delta_step      | Integer >= 0                        | Maximum delta step allowed for each tree's weight estimation. | 0, 1, 5               |\n",
    "| objective           | 'reg:squarederror', 'binary:logistic', etc | Learning task and loss function to use.                   | 'reg:squarederror', 'binary:logistic', etc |\n",
    "| eval_metric         | String                              | Metric used for validation data.                             | 'rmse' (Root Mean Squared Error), 'logloss' (Logarithmic Loss), 'auc' (Area Under Curve) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Regressor\n",
    "\n",
    "| Parameter              | Accepted Values               | Meaning                                                | Common Values         |\n",
    "|------------------------|-------------------------------|--------------------------------------------------------|-----------------------|\n",
    "| iterations            | Integer > 0                   | Controla el número de árboles que se construirán.      | 100, 500, 1000        |\n",
    "| learning_rate         | Float > 0                     | Tasa de aprendizaje del algoritmo de boosting.        | 0.01, 0.03, 0.1       |\n",
    "| depth                 | Integer > 0                   | Profundidad máxima de los árboles.                    | 4, 6, 8               |\n",
    "| l2_leaf_reg  | Float >= 0    | Controla la fuerza de la regularización para evitar el sobreajuste     | 1.0, 3.0, 5.0         |\n",
    "| loss_function         | 'RMSE', 'Logloss', 'MAE', etc| Función de pérdida utilizada para entrenar el modelo. | 'RMSE', 'Logloss'     |\n",
    "| cat_features          | List of integers or None     | Índices de las características categóricas en los datos.| [0, 1, 2], None       |\n",
    "| early_stopping_rounds | Integer >= 0              | Número de iteraciones sin mejora antes de detener el entrenamiento. | 10, 20, None      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "\n",
    "\n",
    "Growht = [ linear | logistic]\n",
    "\n",
    "- Linear = All Increasing, or All decreasing\n",
    "- Logistic = Increasing until one point, then decreasing and continues the loop\n",
    "\n",
    "Cross Validation = [initial, period, horizon]\n",
    "\n",
    "- initial = dates to be loaded (train), can be specified in days, months, minutes, etc.\n",
    "- horizon = the range of dates to be predicted (ex. '30 days')\n",
    "- period =  how many periods you want to predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor (Old)\n",
    "(Old model, better to use XGboost, LightGBM and Catboos)`\n",
    "\n",
    "| Parameter          | Accepted Values                                   | Meaning                                                   | Common Values |\n",
    "|--------------------|---------------------------------------------------|-----------------------------------------------------------|---------------|\n",
    "| loss             | ['huber', 'quantile', 'absolute_error', 'squared_error'] | Determines the loss function used during training.       | 'squared_error' (MSE), 'absolute_error' (MAE) |\n",
    "| learning_rate    | Float > 0                                         | Controls the step size at each iteration of the algorithm. | 0.1, 0.01, 0.001 |\n",
    "| n_estimators     | Integer > 0                                       | Number of boosting stages (trees) to be used.             | 50, 100, ... |\n",
    "| subsample        | Float between 0 and 1                             | Fraction of samples used for fitting the individual trees. | 0.8, 0.9, 1.0 |\n",
    "| criterion        | ['mse', 'friedman_mse', 'mae']                    | Function used to measure the quality of a split.          | 'mse' (MSE), 'mae' (MAE) |\n",
    "| min_samples_split| Integer >= 2                                      | Minimum number of samples required to split a node.        | 2, 5, 10 |\n",
    "| min_samples_leaf | Integer >= 1                                      | Minimum number of samples required to be at a leaf node.   | 1, 5, 10 |\n",
    "| max_depth        | Integer >= 3 or None                              | Maximum depth of the decision trees.                       | 3, 10, None |\n",
    "| max_features     | 'auto', 'sqrt', 'log2', Integer > 0 or None      | Number of features to consider when looking for the best split. | 'auto', 'sqrt', 'log2', 10 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "\n",
    "- loss : \n",
    "\n",
    "    - ['huber', 'quantile', 'absolute_error', 'squared_error']\n",
    "    - Least Squares Loss (`squared_error`): Also known as the Mean Squared Error (MSE), it minimizes the sum of the squares of the differences between the predicted and actual values.\n",
    "    - Least Absolute Deviation (`absolute_error`): Also known as the Mean Absolute Error (MAE), it minimizes the sum of the absolute differences between the predicted and actual values.\n",
    "\n",
    "- learning_rate:\n",
    "    - The learning rate determines the step size at each iteration of the gradient boosting algorithm.\n",
    "    - Common values for the learning rate range from `0.1 to 0.001`, but the optimal value depends on the specific dataset and problem.\n",
    "    - It's often recommended to start with a larger learning rate and gradually decrease it while monitoring the model's performance on a validation set.\n",
    "\n",
    "- n_estimators:\n",
    "    - This is the number of boosting stages (trees) to be used in the ensemble. It's an integer value. More trees will usually improve performance but increase computational cost.\n",
    "    - Common values [50,100 ...]\n",
    "\n",
    "- subsample:\n",
    "    - The subsample parameter in gradient boosting algorithms controls the fraction of samples used for fitting the individual base learners (trees).\n",
    "    - It's a value between 0 and 1, where 1.0 represents the use of all available samples (100%) and values less than 1.0 represent the use of a fraction of samples.\n",
    "    - Setting subsample to less than 1.0 can improve computational efficiency by reducing the training time, especially for large datasets.\n",
    "    -It's often recommended to start with larger values (e.g., 0.8 or 0.9) and gradually decrease the subsample value while monitoring the model's performance on a validation set.\n",
    "\n",
    "- criterion:\n",
    "    - Determines the function used to measure the quality:\n",
    "        - Mean Squared Error (MSE) `'mse'`\n",
    "        - Friedman MSE is an improvement over the traditional MSE criterion and is designed specifically for gradient boosting algorithms\n",
    "        `'friedman_mse'`\n",
    "        - Mean Absolute Error `'mae'`\n",
    "\n",
    "- min_samples_split: This is the minimum number of samples required to split an internal node. Higher values prevent overfitting.\n",
    "    - This prevents the tree from making overly complex and granular splits that capture noise or outliers in the data, reducing the risk of overfitting.\n",
    "    - Common values for min_samples_split range from `2 `(the default) to higher values, such as `5, 10, or even larger`, depending on the size and complexity of the dataset\n",
    "\n",
    "- min_samples_leaf: This is the minimum number of samples required to be at a leaf node. The leaf node is last node that does not split.\n",
    " Higher values prevent overfitting.\n",
    "    - Common values could range from 1 to 5, or even smaller for small datasets while 5 to 20 or higher, depending on the size of the dataset and the desired level of regularization\n",
    "- max_depth: The max_depth parameter determines the maximum number of levels from the root node to the deepest leaf node in a decision tree.\n",
    "    - Common values 3 yo 20, default = None\n",
    "\n",
    "- max_features: This controls the number of features to consider when looking for the best split. Higher values can lead to overfitting, especially when the number of features is high. Gradient Boosting Regression randomly selects a subset of features from the total available features at each node to find the best split.\n",
    "    - 'auto' or 'sqrt' it chooses the square root of the total number of features\n",
    "    - 'log2 it chooses the logarithm of the total number of features\n",
    "    - you can input an manual integer valuer\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
