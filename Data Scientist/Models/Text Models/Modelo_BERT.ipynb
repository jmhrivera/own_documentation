{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrando Modelo BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm_he\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('D:/Tripleten/datasets/imdb_reviews_small.tsv' ,sep='\\t')\n",
    "corpus = data['review']\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "for input_text in corpus[:200]:\n",
    "    ids = tokenizer.encode(\n",
    "        input_text.lower(),\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    padded = np.array(ids + [0] * (max_length - len(ids)))\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    ids_list.append(padded)\n",
    "    attention_mask_list.append(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2ed64fdb3c49eb8802ab9706d7d7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inicialización de la configuracíon de BertConfig\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#Inicializando el modelo\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tqdm es una biblioteca de Python que te permite agregar barras de progreso a tus bucles o iteraciones.\n",
    "for i in tqdm(range(int(8e6))):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81398603a00d43ae8bbcb80ca4d8217d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jm_he\\AppData\\Local\\Temp\\ipykernel_1312\\3713879363.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  ids_batch = torch.LongTensor(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# creación de una lista vacia de embeddings de reseñas\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(len(ids_list) // batch_size)):\n",
    "\n",
    "    ids_batch = torch.LongTensor(\n",
    "        ids_list[batch_size * i : batch_size * (i + 1)]\n",
    "    )\n",
    "    attention_mask_batch = torch.LongTensor(\n",
    "        attention_mask_list[batch_size * i : batch_size * (i + 1)]\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(\n",
    "            ids_batch, attention_mask=attention_mask_batch\n",
    "        )\n",
    "    embeddings.append(batch_embeddings[0][:, 0, :].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.122169</td>\n",
       "      <td>0.092195</td>\n",
       "      <td>-0.249359</td>\n",
       "      <td>-0.055086</td>\n",
       "      <td>-0.228724</td>\n",
       "      <td>-0.551546</td>\n",
       "      <td>-0.114361</td>\n",
       "      <td>0.324784</td>\n",
       "      <td>0.209913</td>\n",
       "      <td>-0.026322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164391</td>\n",
       "      <td>-0.184356</td>\n",
       "      <td>0.113958</td>\n",
       "      <td>-0.069024</td>\n",
       "      <td>-0.452524</td>\n",
       "      <td>0.564213</td>\n",
       "      <td>-0.142874</td>\n",
       "      <td>0.145212</td>\n",
       "      <td>0.560320</td>\n",
       "      <td>0.688981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.549666</td>\n",
       "      <td>-0.151823</td>\n",
       "      <td>-0.416533</td>\n",
       "      <td>-0.066173</td>\n",
       "      <td>-0.344536</td>\n",
       "      <td>-0.474458</td>\n",
       "      <td>0.144660</td>\n",
       "      <td>0.084561</td>\n",
       "      <td>0.363413</td>\n",
       "      <td>-0.095575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024756</td>\n",
       "      <td>-0.341043</td>\n",
       "      <td>0.102159</td>\n",
       "      <td>-0.182383</td>\n",
       "      <td>-0.259226</td>\n",
       "      <td>0.807761</td>\n",
       "      <td>0.026817</td>\n",
       "      <td>0.068977</td>\n",
       "      <td>0.819748</td>\n",
       "      <td>0.512458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.113625</td>\n",
       "      <td>0.067533</td>\n",
       "      <td>-0.326989</td>\n",
       "      <td>0.322551</td>\n",
       "      <td>-0.487812</td>\n",
       "      <td>-0.477263</td>\n",
       "      <td>-0.066992</td>\n",
       "      <td>0.348855</td>\n",
       "      <td>-0.191485</td>\n",
       "      <td>-0.368260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085881</td>\n",
       "      <td>-0.240073</td>\n",
       "      <td>0.278587</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>-0.378652</td>\n",
       "      <td>0.827314</td>\n",
       "      <td>-0.226071</td>\n",
       "      <td>-0.279679</td>\n",
       "      <td>0.667773</td>\n",
       "      <td>0.425052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.367855</td>\n",
       "      <td>-0.305474</td>\n",
       "      <td>0.169522</td>\n",
       "      <td>0.067513</td>\n",
       "      <td>-0.218734</td>\n",
       "      <td>-0.300824</td>\n",
       "      <td>0.067415</td>\n",
       "      <td>0.890659</td>\n",
       "      <td>-0.066575</td>\n",
       "      <td>-0.517766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061854</td>\n",
       "      <td>-0.277576</td>\n",
       "      <td>-0.227234</td>\n",
       "      <td>-0.286016</td>\n",
       "      <td>0.106644</td>\n",
       "      <td>0.348105</td>\n",
       "      <td>-0.243855</td>\n",
       "      <td>-0.098266</td>\n",
       "      <td>0.872936</td>\n",
       "      <td>0.404907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111954</td>\n",
       "      <td>0.005557</td>\n",
       "      <td>0.481005</td>\n",
       "      <td>-0.228786</td>\n",
       "      <td>-0.275876</td>\n",
       "      <td>-0.493747</td>\n",
       "      <td>0.554269</td>\n",
       "      <td>0.407132</td>\n",
       "      <td>0.089381</td>\n",
       "      <td>-0.384155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086957</td>\n",
       "      <td>-0.352565</td>\n",
       "      <td>-0.196727</td>\n",
       "      <td>-0.390338</td>\n",
       "      <td>-0.008241</td>\n",
       "      <td>0.261729</td>\n",
       "      <td>-0.164578</td>\n",
       "      <td>-0.314623</td>\n",
       "      <td>0.533540</td>\n",
       "      <td>0.558253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.006953</td>\n",
       "      <td>-0.173385</td>\n",
       "      <td>-0.140757</td>\n",
       "      <td>-0.401268</td>\n",
       "      <td>-0.285115</td>\n",
       "      <td>-0.582246</td>\n",
       "      <td>0.034676</td>\n",
       "      <td>0.883379</td>\n",
       "      <td>0.091907</td>\n",
       "      <td>-0.569235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026106</td>\n",
       "      <td>-0.688050</td>\n",
       "      <td>0.193886</td>\n",
       "      <td>-0.236663</td>\n",
       "      <td>-0.363652</td>\n",
       "      <td>0.372739</td>\n",
       "      <td>-0.226602</td>\n",
       "      <td>-0.341709</td>\n",
       "      <td>0.557226</td>\n",
       "      <td>0.403866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.060361</td>\n",
       "      <td>0.176036</td>\n",
       "      <td>-0.302104</td>\n",
       "      <td>-0.164362</td>\n",
       "      <td>-0.498970</td>\n",
       "      <td>0.080711</td>\n",
       "      <td>1.008229</td>\n",
       "      <td>0.259629</td>\n",
       "      <td>-0.725750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048318</td>\n",
       "      <td>-0.280682</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>-0.379457</td>\n",
       "      <td>0.112021</td>\n",
       "      <td>0.135773</td>\n",
       "      <td>-0.040638</td>\n",
       "      <td>0.086089</td>\n",
       "      <td>0.262679</td>\n",
       "      <td>0.498620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.120240</td>\n",
       "      <td>-0.149923</td>\n",
       "      <td>0.086899</td>\n",
       "      <td>-0.340308</td>\n",
       "      <td>-0.211053</td>\n",
       "      <td>-0.274579</td>\n",
       "      <td>0.329264</td>\n",
       "      <td>0.624461</td>\n",
       "      <td>0.226524</td>\n",
       "      <td>-0.202640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049597</td>\n",
       "      <td>-0.658827</td>\n",
       "      <td>-0.070280</td>\n",
       "      <td>-0.128696</td>\n",
       "      <td>-0.059341</td>\n",
       "      <td>0.252980</td>\n",
       "      <td>-0.199175</td>\n",
       "      <td>-0.176068</td>\n",
       "      <td>0.390736</td>\n",
       "      <td>0.463716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.236308</td>\n",
       "      <td>-0.127540</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>-0.105859</td>\n",
       "      <td>-0.036008</td>\n",
       "      <td>-0.583316</td>\n",
       "      <td>0.388649</td>\n",
       "      <td>1.011410</td>\n",
       "      <td>0.035036</td>\n",
       "      <td>-0.892944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081190</td>\n",
       "      <td>-0.585537</td>\n",
       "      <td>0.182939</td>\n",
       "      <td>-0.123127</td>\n",
       "      <td>-0.508081</td>\n",
       "      <td>1.004613</td>\n",
       "      <td>-0.230666</td>\n",
       "      <td>-0.649622</td>\n",
       "      <td>0.730459</td>\n",
       "      <td>-0.051130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.115554</td>\n",
       "      <td>-0.040958</td>\n",
       "      <td>0.024316</td>\n",
       "      <td>-0.385589</td>\n",
       "      <td>-0.135446</td>\n",
       "      <td>-0.429562</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.945266</td>\n",
       "      <td>-0.386388</td>\n",
       "      <td>-0.429422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346288</td>\n",
       "      <td>-0.365461</td>\n",
       "      <td>-0.062882</td>\n",
       "      <td>-0.163163</td>\n",
       "      <td>-0.241082</td>\n",
       "      <td>1.140705</td>\n",
       "      <td>-0.183832</td>\n",
       "      <td>-0.673455</td>\n",
       "      <td>0.663204</td>\n",
       "      <td>0.041120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.122169  0.092195 -0.249359 -0.055086 -0.228724 -0.551546 -0.114361   \n",
       "1   -0.549666 -0.151823 -0.416533 -0.066173 -0.344536 -0.474458  0.144660   \n",
       "2   -0.113625  0.067533 -0.326989  0.322551 -0.487812 -0.477263 -0.066992   \n",
       "3    0.367855 -0.305474  0.169522  0.067513 -0.218734 -0.300824  0.067415   \n",
       "4    0.111954  0.005557  0.481005 -0.228786 -0.275876 -0.493747  0.554269   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  0.006953 -0.173385 -0.140757 -0.401268 -0.285115 -0.582246  0.034676   \n",
       "196  0.320755  0.060361  0.176036 -0.302104 -0.164362 -0.498970  0.080711   \n",
       "197  0.120240 -0.149923  0.086899 -0.340308 -0.211053 -0.274579  0.329264   \n",
       "198 -0.236308 -0.127540  0.010173 -0.105859 -0.036008 -0.583316  0.388649   \n",
       "199  0.115554 -0.040958  0.024316 -0.385589 -0.135446 -0.429562  0.322700   \n",
       "\n",
       "          7         8         9    ...       758       759       760  \\\n",
       "0    0.324784  0.209913 -0.026322  ... -0.164391 -0.184356  0.113958   \n",
       "1    0.084561  0.363413 -0.095575  ... -0.024756 -0.341043  0.102159   \n",
       "2    0.348855 -0.191485 -0.368260  ... -0.085881 -0.240073  0.278587   \n",
       "3    0.890659 -0.066575 -0.517766  ... -0.061854 -0.277576 -0.227234   \n",
       "4    0.407132  0.089381 -0.384155  ... -0.086957 -0.352565 -0.196727   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "195  0.883379  0.091907 -0.569235  ...  0.026106 -0.688050  0.193886   \n",
       "196  1.008229  0.259629 -0.725750  ... -0.048318 -0.280682  0.075472   \n",
       "197  0.624461  0.226524 -0.202640  ...  0.049597 -0.658827 -0.070280   \n",
       "198  1.011410  0.035036 -0.892944  ...  0.081190 -0.585537  0.182939   \n",
       "199  0.945266 -0.386388 -0.429422  ...  0.346288 -0.365461 -0.062882   \n",
       "\n",
       "          761       762       763       764       765       766       767  \n",
       "0   -0.069024 -0.452524  0.564213 -0.142874  0.145212  0.560320  0.688981  \n",
       "1   -0.182383 -0.259226  0.807761  0.026817  0.068977  0.819748  0.512458  \n",
       "2   -0.163645 -0.378652  0.827314 -0.226071 -0.279679  0.667773  0.425052  \n",
       "3   -0.286016  0.106644  0.348105 -0.243855 -0.098266  0.872936  0.404907  \n",
       "4   -0.390338 -0.008241  0.261729 -0.164578 -0.314623  0.533540  0.558253  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195 -0.236663 -0.363652  0.372739 -0.226602 -0.341709  0.557226  0.403866  \n",
       "196 -0.379457  0.112021  0.135773 -0.040638  0.086089  0.262679  0.498620  \n",
       "197 -0.128696 -0.059341  0.252980 -0.199175 -0.176068  0.390736  0.463716  \n",
       "198 -0.123127 -0.508081  1.004613 -0.230666 -0.649622  0.730459 -0.051130  \n",
       "199 -0.163163 -0.241082  1.140705 -0.183832 -0.673455  0.663204  0.041120  \n",
       "\n",
       "[200 rows x 768 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.concatenate(embeddings) #No es real, tengo que obtenerlo de otro lado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=1/2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train,y_train)\n\u001b[0;32m      3\u001b[0m predict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m----> 4\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:105\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    108\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n",
      "\u001b[1;31mValueError\u001b[0m: continuous-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "predict = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test,predict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
