{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de Normalización de Texto en NPL\n",
    "\n",
    "- Stemming: Reglas fijas como descartar`able,ing` para derivar una palabra\n",
    "- Lemmatization: Uso del `conocimiento del lenguaje` (a.k.a. linguistic knowledge) to derive a word\n",
    "\n",
    "\n",
    "### Pasos para el preprocesamiento de texto:\n",
    "\n",
    "1. Tokenización: dividir el texto en tokens (frases, palabras y símbolos separados)\n",
    "2. Lematización: reducir las palabras a sus formas fundamentales (lema)\n",
    "3. BoW\n",
    "\n",
    "#### Librerias para tokenizar y lematizar:\n",
    "- Natural Language Toolkit (NLTK) - (Uso de bolsa de palabras)\n",
    "- spaCy (Uso de bolsa de palabras) o TF-IDF\n",
    "- word2vec - (embedded o insertado de palabra)\n",
    "- etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento usando NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es el texto tokenizado\n",
      "['all', 'models', 'are', 'wrong', ',', 'but', 'some', 'are', 'useful']\n",
      "\n",
      "Esto es el texto lematizado\n",
      "['all', 'model', 'are', 'wrong', ',', 'but', 'some', 'are', 'useful']\n",
      "\n",
      "Texto procesado\n",
      "all model are wrong , but some are useful\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Llamamos a la clase wordnetlematizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text= 'All models are wrong, but some are useful'\n",
    "\n",
    "# Tokenizamos nuestro texto que subdvide el texto\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Derivamos nuestros tokens\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "doc = \" \".join(lemmas)\n",
    "\n",
    "print('Esto es el texto tokenizado')\n",
    "print(tokens, end='\\n\\n')\n",
    "\n",
    "print('Esto es el texto lematizado')\n",
    "print(lemmas, end='\\n\\n')\n",
    "\n",
    "print('Texto procesado')\n",
    "print(doc, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento usando Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all model be wrong , but some be useful\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#Natural language processor\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "text= 'All models are wrong, but some are useful'\n",
    "doc = nlp(text.lower())\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento spacy con un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "10\n",
      "[\"this true story of carlson 's raider be more of a army training film than anything else.obviously throw together quickly on a miniscule budget about the only thing it have to recommend it be an early performance by robert mitchum , who be the only decent actor in the cast , and actual footage of the wreckage at pearl harbor which get your blood boiling , as it be obviously intend to do .\", \"should have be title ' balderdash ! ' little in the film be true except the name of the island and the fact submarine be involve . little more than train film quality with poor camera work , muddy stock footage and perhaps the low point of stereotype ' jap ' with laugh japanese infantry , laugh japanese fighter pilot and one - dimensional square - jaw americans die leave and right . sixty year later it be unintentionally funny as an odd artifact and as an opportunity to see what be possible when the war fever be upon you . the plot and the dialogue remind I of play gun on a summer 's afternoon in my childhood , peer through the neighbor 's hedge to gain a fatal advantage on my good friend steve and my little brother . in actual fact , the makin island raid be a near total failure with carlson and his man wander around in the dark exchange gunfire with shadow until finally , thirsty and completely disorient , look for someone to surrender to , before they happen upon some equally confused japanese soldier who promptly surrender to they ! in the withdrawal several of carlson 's marine end up on another island and be abandon ! the film , of course , could not tell that story , not in 1943 , so this bit of whimsy be fabricate and rush into release to the beating of drum . with randolph scott , and his jaw , as colonel thorwald ( carlson ) lead a unit comprise almost entirely of stock caricature , the green recruit ( harry landon , robert mitchum ) , the grizzled veteran ( j. carroll naish , milburn stone , sam levene ) , the country - bumpkin ( rod cameron ) , the all - american boy ( alan curtis ) , and score of sneer ( when they be not laugh ) ' japs ' . and yet the cast nearly overcome the material . almost . randolph scott 's narrow range be well suited to his role of earnest commander and he be support by a solid group of professional who do their good with thin gruel . but in the end , the one - note object of the exercise win . any pretense be totally abandon at the close when randy scott simply look directly into the camera and deliver a stirring ( well sorta stirring ) call to arm . the cast be well than this material . so be the audience . should be view with reefer madness and a bottle of moderately price merlot .\", \"the movie ' gung ho ! ' : the story of carlson 's makin island raider be make in 1943 with a view to go up the moral of american people at the duration of second world war . it show with the well way that the cinema can constitute body of propaganda . the value of this film be only collection and no artistic . in a film of propaganda it be useless to judge direction and actor . watch that movie if you be interested to learn how propaganda function in the movie or if you be a big fun of robert mitchum who have a small role in the film . if you want to see a film for the second world war , they exist much well and objective . I rate it 4/10 .\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "data = pd.read_csv('D:/Tripleten/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "corpus = data['review'][0:10] # Hacemos mas corto nuestro dataframe para que se tarde menos\n",
    "\n",
    "lemmas_list = []\n",
    "\n",
    "def lemmatize(corpus):\n",
    "    for text in corpus:\n",
    "        doc= nlp(text.lower())\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        lemmas_list.append(\" \".join(lemmas))\n",
    "    return lemmas_list\n",
    "\n",
    "\n",
    "lemmatize(corpus)\n",
    "\n",
    "print(corpus.shape)\n",
    "print(len(lemmas_list))\n",
    "print(lemmas_list[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expresiones Regulares\n",
    "\n",
    "Dentro de la libreria re, podemos hacer uso de la función que nos permitirá encontrar una busqueda y remplazarla re.sub(pattern, substitution, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked this show from the first episode I saw, which was the \"Rhapsody in Blue\" episode (for those that don't know what that is, the Zan going insane and becoming pau lvl 10 ep). Best visuals and special effects I've seen on a television series, nothing like it anywhere.\n",
      "I liked this show from the first episode I saw which was the Rhapsody in Blue episode for those that don't know what that is the Zan going insane and becoming pau lvl ep Best visuals and special effects I've seen on a television series nothing like it anywhere\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = '''I liked this show from the first episode I saw, which was the \"Rhapsody in Blue\" episode (for those that don't know what that is, the Zan going insane and becoming pau lvl 10 ep). Best visuals and special effects I've seen on a television series, nothing like it anywhere.'''\n",
    "\n",
    "# re.sub(pattern, substitution, text)\n",
    "clear_text = re.sub(r'[^A-Za-z\\' ]', '', text) #Buscamos palabras que sean diferentes de A hasta la Z en mayusculas y minúsculas, es decir, puntos y comas, los sustituiremos por nada. \n",
    "\n",
    "clear_text = \" \".join(clear_text.split())  # es muy importante aplicar el split join porque algunas veces se quedan dobles espacios por los elementos que estamos descartando\n",
    "    \n",
    "print(text)\n",
    "print(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos una función para limpiar los acentos y puntoss antes de entregar nuestro texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday I found an ugly dog don't you      think \n",
      "Yesterday I found an ugly dog don't you think\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean(text):\n",
    "    '''Eliminamos todo lo que sea diferente a A-Z mayusculas y minusculas, \n",
    "    y mantenemos espacios y apostrofes. Lo demás será eliminado.'''\n",
    "    clear_text = re.sub(r'[^A-Za-z\\' ]',' ', text) \n",
    "    clean_text = \" \".join(clear_text.split())\n",
    "    return clear_text, clean_text\n",
    "\n",
    "to_clean,clear = clean(\"Yesterday I found an ugly dog don't you      think?\")\n",
    "\n",
    "print(to_clean)\n",
    "print(clear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora pasaremos todo lo aprendido a un solo apartado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this true story of carlson 's raider be more of a army training film than anything else.obviously throw together quickly on a miniscule budget about the only thing it have to recommend it be an early performance by robert mitchum , who be the only decent actor in the cast , and actual footage of the wreckage at pearl harbor which get your blood boiling , as it be obviously intend to do .\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#Corregimoss el formato de else obviously\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmas_list[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 32\u001b[0m \u001b[43mclean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmas_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Podríamos mostrar la lista completa pero queremos hacer enfasis en lo que se logra.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m, in \u001b[0;36mclean\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean\u001b[39m(text):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Eliminamos todo lo que sea diferente a A-Z mayusculas y minusculas, \u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    y mantenemos espacios y apostrofes. Lo demás será eliminado.'''\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     clear_text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^A-Za-z\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m ]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text) \n\u001b[0;32m     16\u001b[0m     clean_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(clear_text\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clean_text\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "data = pd.read_csv('D:/Tripleten/datasets/imdb_reviews_small.tsv', sep='\\t')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "corpus = data['review'][0:10] # Hacemos mas corto nuestro dataframe para que se tarde menos\n",
    "\n",
    "lemmas_list = []\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    '''Eliminamos todo lo que sea diferente a A-Z mayusculas y minusculas, \n",
    "    y mantenemos espacios y apostrofes. Lo demás será eliminado.'''\n",
    "    clear_text = re.sub(r'[^A-Za-z\\' ]',' ', text) \n",
    "    clean_text = \" \".join(clear_text.split())\n",
    "    return clean_text\n",
    "\n",
    "def lemmatize(corpus):\n",
    "    for text in corpus:\n",
    "        doc= nlp(text.lower())\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        lemmas_list.append(\" \".join(lemmas))\n",
    "    return lemmas_list\n",
    "\n",
    "\n",
    "lemmatize(corpus)\n",
    "\n",
    "\n",
    "#Corregimoss el formato de else obviously\n",
    "print(lemmas_list[0])\n",
    "clean(lemmas_list[0]) # Podríamos mostrar la lista completa pero queremos hacer enfasis en lo que se logra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words (BoW)\n",
    "\n",
    "Ahora vamos a aprender cómo podemos convertir datos de texto en datos numéricos, Lo que hace es transformar textos en vectores sin tomar en cuenta el orden de las palabras y, por eso, se llama bolsa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW Usando libreria spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'for': 3, 'want': 3, 'of': 3, 'a': 3, 'the': 3, 'be': 3, 'lose': 3, 'shoe': 2, 'horse': 2, 'nail': 1, 'rider': 1})\n",
      "dict_keys(['for', 'want', 'of', 'a', 'nail', 'the', 'shoe', 'be', 'lose', 'horse', 'rider'])\n",
      "[3, 3, 3, 2, 3, 1, 3, 1, 2, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])\n",
    "text =  \"\"\"For want of a nail the shoe was lost. For want of a' shoe the horse was \"lost\". For want of a horse the rider was lost.\"\"\"\n",
    "doc = nlp(text)\n",
    "tokens = [token.lemma_ for token in doc if not token.is_punct] #token.is_punct excluye los signos de puntuacion.\n",
    "bow = Counter(tokens) #Cuenta el número de cada palabra,\n",
    "\n",
    "vector = [bow[token] for token in sorted(bow)]\n",
    "\n",
    "print(bow) # Obtenemos los valores y diccionarios\n",
    "print (bow.keys()) # Obtenemos los nombress de los diccionarios\n",
    "print(vector) # Imprimimos nuestro vector creado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # n-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un n-grama es una secuencia de varias palabras. N indica el número de elementos y es arbitrario. Por ejemplo, si N=1, tenemos palabras separadas o unigramas. Si N=2, tenemos frases de dos palabras o bigramas. N=3 produce trigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW Usando libreria Scikit Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tamaño (7, 16): 7 textos, 16 palabras únicas\n",
      "\n",
      "El vocabulario es el siguiente: ['all' 'and' 'battle' 'be' 'for' 'horse' 'horseshoe' 'kingdom' 'lose'\n",
      " 'message' 'nail' 'of' 'rider' 'shoe' 'the' 'want']\n",
      "[[0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1]\n",
      " [0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1]\n",
      " [0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1]\n",
      " [1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer() #Count vecotizer no toma en cuenta letras individuales.\n",
    "\n",
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]\n",
    "\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "unique_words = count_vect.get_feature_names_out()\n",
    "\n",
    "print(f' Tamaño {bow.shape}: {bow.shape[0]} textos, {bow.shape[1]} palabras únicas', end='\\n\\n')\n",
    "print(f'El vocabulario es el siguiente: {unique_words}')\n",
    "print(bow.toarray(), end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW Usando libreria Scikit Learn y n-gramas = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisieramos hacer una bolsa de palabras con 2 o más elementos podemos usar el parametro ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vocabulario es el siguiente: ['all for' 'and all' 'battle be' 'battle the' 'be lose' 'for the'\n",
      " 'for want' 'horse be' 'horse the' 'horseshoe nail' 'kingdom be'\n",
      " 'message be' 'message the' 'nail the' 'of battle' 'of horse'\n",
      " 'of horseshoe' 'of message' 'of nail' 'of rider' 'of shoe' 'rider be'\n",
      " 'rider the' 'shoe be' 'shoe the' 'the battle' 'the horse' 'the kingdom'\n",
      " 'the message' 'the rider' 'the shoe' 'the want' 'want of']\n",
      "[[0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1]\n",
      " [1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]]\n",
      "(7, 33)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(2,2))\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "unique_words = count_vect.get_feature_names_out()\n",
    "print(f'El vocabulario es el siguiente: {unique_words}')\n",
    "print(bow.toarray())\n",
    "print(bow.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluir palabras vacias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas veces deseamos excluir palabras vacias (que por si solas no aportan nada), para poder excluirlas podemos importarlas de la libreria de nltk y mandarselas como parametro a nuestro CounVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tamaño (7, 10): 7 textos, 10 palabras únicas\n",
      "El vocabulario es el siguiente: ['battle' 'horse' 'horseshoe' 'kingdom' 'lose' 'message' 'nail' 'rider'\n",
      " 'shoe' 'want']\n",
      "[[0 0 0 0 1 0 1 0 1 1]\n",
      " [0 1 0 0 1 0 0 0 1 1]\n",
      " [0 1 0 0 1 0 0 1 0 1]\n",
      " [0 0 0 0 1 1 0 1 0 1]\n",
      " [1 0 0 0 1 1 0 0 0 1]\n",
      " [1 0 0 1 1 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords') # Para descargar la última versión de stopwords\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=stop_words) #Count vectizer no toma en cuenta letras individuales.\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    'for want of a nail the shoe be lose',\n",
    "    'for want of a shoe the horse be lose',\n",
    "    'for want of a horse the rider be lose',\n",
    "    'for want of a rider the message be lose',\n",
    "    'for want of a message the battle be lose',\n",
    "    'for want of a battle the kingdom be lose',\n",
    "    'and all for the want of a horseshoe nail'\n",
    "]\n",
    "\n",
    "bow = count_vect.fit_transform(corpus)\n",
    "unique_words = count_vect.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(f' Tamaño {bow.shape}: {bow.shape[0]} textos, {bow.shape[1]} palabras únicas')\n",
    "print(f'El vocabulario es el siguiente: {unique_words}')\n",
    "print(bow.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La lista de palabras únicas en la bolsa se llama vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IFD \n",
    "La formula TF-IDF mejor conocida como Term Frequency , Inverse Document Frequency, se ejecuta de la siguiente forma.\n",
    "\n",
    "$$ TF-IDF  - TF * IDF $$\n",
    "\n",
    "En donde: \n",
    "\n",
    "$TF = \\frac{f}{n}$\n",
    "\n",
    "- f = frecuencia que aparece la palabra\n",
    "- n = numero total de palabras\n",
    "\n",
    "$IDF = log_{10}(\\frac{D}{d})$\n",
    "\n",
    "- D = Numero de parrafos\n",
    "- d = Numero de parrafos en donde aparece la palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando TF-IDF con Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño de la matriz TF-IDF: (4541, 26098)\n",
      "['aa' 'aaaaaaaaaaaahhhhhhhhhhhhhh' 'aaah' ... 'zuucka' 'zuzz' 'zz']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "data = pd.read_csv('D:/Tripleten/datasets/imdb_reviews_small_lemm.tsv', sep='\\t')\n",
    "corpus = data['review_lemm']\n",
    "\n",
    "stop_words = list(set(nltk_stopwords.words('english')))\n",
    "tfid_vect = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "tf_idf = tfid_vect.fit_transform(corpus)\n",
    "vocabulary = tfid_vect.get_feature_names_out()\n",
    "\n",
    "print('El tamaño de la matriz TF-IDF:', tf_idf.shape)\n",
    "print(vocabulary)\n",
    "\n",
    "# x = pd.DataFrame(tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Análisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente ejercicio incluye el uso de creación de bolsas para ser usadas en un modelo de regresion. Las bolsas por si solas no mencionan si algo es negativo o positivo es por eso que la columna [pos] entrenará el modelo y le enseñara que elementos han sido considerados como negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El resultado accuracy_score es 0.8757396449704142\n",
      "El resultado f1_score es 0.9007874015748032\n",
      "El resultado roc_score es 0.857286576602472\n",
      "      pos\n",
      "0       0\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       0\n",
      "...   ...\n",
      "2215    0\n",
      "2216    1\n",
      "2217    1\n",
      "2218    1\n",
      "2219    1\n",
      "\n",
      "[2220 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, root_mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('D:/Tripleten/datasets/imdb_reviews_small_lemm_train.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('D:/Tripleten/datasets/imdb_reviews_small_lemm_test.tsv', sep='\\t')\n",
    "\n",
    "train_corpus = train_data['review_lemm']# extraer reseñas lematizadas para el entrenamiento\n",
    "\n",
    "stop_words = list(set(nltk_stopwords.words('english')))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "tf_idf = count_tf_idf.fit_transform(train_corpus) # Transformando mis texto a vectores de palabras únicas.\n",
    "\n",
    "features_train = tf_idf \n",
    "target_train = train_data['pos']# extraer la columna objetivo\n",
    "\n",
    "test_corpus = test_data['review_lemm'] # extraer reseñas lematizadas para la prueba\n",
    "features_test = count_tf_idf.transform(test_corpus) # transformar el corpus de entrenamiento\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(features_train, target_train ,test_size=1/4 , random_state=12345)\n",
    "\n",
    "model =  LogisticRegression() # inicializar el modelo de regresión logística y ajustarlo\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred_pre_test = model.predict(X_test)# obtener predicciones para la parte de prueba de los datos\n",
    "acc_score = accuracy_score(y_test,pred_pre_test) \n",
    "f1score = f1_score(y_test,pred_pre_test)\n",
    "roc_score = roc_auc_score(y_test,pred_pre_test)\n",
    "# rmse = root_mean_squared_error(y_test,pred_pre_test) #No es usado para clasificación\n",
    "\n",
    "\n",
    "print(f'El resultado accuracy_score es {acc_score}')\n",
    "print(f'El resultado f1_score es {f1score}')\n",
    "print(f'El resultado roc_score es {roc_score}')\n",
    "# print(f'El resultado rmse es {rmse}')\n",
    "\n",
    "\n",
    "pred_test = model.predict(features_test)# obtener predicciones para la parte de prueba de los datos\n",
    "\n",
    "# transformar las predicciones en un DataFrame y guardarlo\n",
    "submission = pd.DataFrame({'pos':pred_test})\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasos para el preprocesamiento\n",
    "\n",
    "Vamos a repasar los detalles.\n",
    "\n",
    "1. Antes de pasar a la vectorización de palabras, necesitaremos realizar un preprocesamiento:\n",
    "- Cada texto está tokenizado (descompuesto en palabras).\n",
    "- Luego se lematizan las palabras (reducidas a su forma raíz). Sin embargo, los modelos más complejos, como BERT, no requieren este paso - porque entienden las formas de las palabras.\n",
    "- El texto se limpia de palabras vacías o caracteres innecesarios.\n",
    "- Para algunos algoritmos (por ejemplo, BERT), se agregan tokens especiales para marcar el comienzo y el final de las oraciones.\n",
    "\n",
    "2. Cada texto adquiere su propia lista de tokens después del preprocesamiento.\n",
    "\n",
    "3. Luego, los tokens se pasan al modelo, que los vectoriza mediante el uso de un vocabulario de tokens precompilado. En la salida obtenemos - vectores de longitud predeterminada formados para cada texto.\n",
    "\n",
    "4. El paso final es pasar las características (vectores) al modelo. Luego, el modelo predice la tonalidad del texto: \"0\" — negativo o \"1\" — positivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embedded` (Insertado de palabra) = Embedded en el contexto de procesamiento de lenguaje natural (NLP) significa que una palabra está representada como un vector numérico en un espacio vectorial que forma parte de un modelo de lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "Word2vec es un método que se utiliza para convertir palabras en vectores (de ahí su nombre) para que las palabras cercanas semánticamente acerquen los vectores entre sí.\n",
    "\n",
    "Por lo tanto, para obtener vectores relevantes para tus textos, debes seleccionar un modelo word2vec que se haya creado para un corpus de textos semánticamente relevante. Por ejemplo, para convertir noticias en vectores, se debe usar un modelo word2vec entrenado en un corpus de noticias.\n",
    "\n",
    "Además de convertir palabras en vectores, los modelos word2vec también se pueden usar para resolver tareas específicas de PNL. Por ejemplo, pueden ayudar a predecir si algunas palabras son vecinas o no. Las palabras se consideran vecinas si caen en la misma \"ventana\" (distancia máxima entre palabras)\n",
    "\n",
    "Otra cosa que puede hacer word2vec es entrenar un modelo para distinguir pares de vecinas verdaderas de las aleatorias. Esta tarea es como una tarea de clasificación binaria, donde las características son palabras y el objetivo es la respuesta a la pregunta de si las palabras son vecinas verdaderas o no.\n",
    "\n",
    "¿Cómo encontramos los pares de palabras vecinas para entrenar word2vec?\n",
    "Encontramos los n-gramas a lo largo de todo el corpus de textos.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Bert\n",
    "\n",
    "El modelo BERT (Bidirectional Encoder Representation Transsformers) puede ayudarnos a resolver esto de manera mas sencilla ya que incorpora bastantes elementos en su modelo.\n",
    "\n",
    "BERT es un paso evolutivo en comparación con word2vec. BERT se convirtió rápidamente en la opción popular para los programadores y ha inspirado a los investigadores a crear otros modelos de representación de lenguaje: FastText, GloVe (Vectores globales para representación de palabras), ELMO (Embeddings from Language Models), GPT (Generative Pre-trained Transformer). Los modelos más precisos actualmente son BERT y GPT.\n",
    "\n",
    "Al procesar palabras, BERT considera tanto las palabras vecinas inmediatas como las palabras más lejanas. Esto permite que BERT produzca vectores precisos con respecto al significado natural de las palabras.\n",
    "\n",
    "berti tiene su propio tokenizador y no requiere lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    " Tenemos un gran conjunto de datos de reseñas de películas y necesitamos entrenar la máquina para diferenciar entre reseñas positivas y negativas.\n",
    "\n",
    "Vamos a resolver esta tarea usando las librerías PyTorch y transformers. La primera librería se utiliza para trabajar con modelos de redes neuronales, mientras que la segunda implementa BERT y otros modelos de representación del lenguaje. Vamos a importarlas:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estos son indices: [101, 9765, 2226, 6767, 9202, 2474, 21877, 10415, 7068, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "example = 'Estuvo horrible la película.'\n",
    "\n",
    "#ids = vectores de identificadores numéricos de tokens\n",
    "ids = tokenizer.encode(example, add_special_tokens=True)\n",
    "\n",
    "\n",
    "print(f'Estos son indices: {ids}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para operar el modelo correctamente, establecemos el argumento add_`special_tokens en True`. Significa que agregamos el token inicial (101) y el token final (102) a cualquier texto que se esté transformando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estos son indices: [101, 9765, 2226, 6767, 9202, 2474, 21877, 10415, 7068, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "example = 'Estuvo horrible la película.'\n",
    "ids = tokenizer.encode(example, add_special_tokens=True)\n",
    "\n",
    "\n",
    "# BERT acepta vectores de longitud fija, por ejemplo 20 tokens. Si faltan, se rellenan con 0, si sobran se\n",
    "#  limitará hasta n-2 (inicio con 101 y fin con 102) unidades\n",
    "n= 512 # Longitud maxima de BERT base en tokens únicos\n",
    "padded = np.array(ids[:n]+[0]*(n-len(ids)))\n",
    "\n",
    "# print(padded)\n",
    "print(f'Estos son indices: {ids}')\n",
    "# print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mascaras (Attention Mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que decirle al modelo por qué los ceros no tienen información significativa. Esto es necesario para el componente del modelo que se llama attention. Vamos a descartar estos tokens y crear una máscara para los tokens importantes, indicando valores cero y distintos de cero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "[  101  9765  2226  6767  9202  2474 21877 10415  7068  1012   102     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "example = 'Estuvo horrible la película.'\n",
    "ids = tokenizer.encode(example, add_special_tokens=True)\n",
    "\n",
    "# BERT acepta vectores de longitud fija, por ejemplo 20 tokens. Si faltan, se rellenan con 0, si sobran se\n",
    "#  limitará hasta n-2 (inicio con 101 y fin con 102) unidades\n",
    "n= 20\n",
    "padded = np.array(ids[:n]+[0]*(n-len(ids)))\n",
    "\n",
    "# print(padded)\n",
    "# print(f'Estos son indices: {ids}')\n",
    "\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print(attention_mask.shape)\n",
    "\n",
    "\n",
    "# Datos reales.\n",
    "print(padded)\n",
    "\n",
    "# Attention Mask\n",
    "print(attention_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opción 2\n",
    "- Podemos truncar los valores maximos con los parametros max_length=n y truncation=True en caso de necesitar limitar, ya que BERT base solo procesa vectores de longitud maxima de 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9765, 2226, 6767, 9202, 2474, 21877, 10415, 7068, 1012, 102]\n",
      "[  101  9765  2226  6767  9202  2474 21877 10415  7068  1012   102     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "n= 512\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "example = 'Estuvo horrible la película.'\n",
    "ids = tokenizer.encode(example, add_special_tokens=True, max_length=n, truncation=True)\n",
    "\n",
    "\n",
    "# BERT acepta vectores de longitud fija, por ejemplo 20 tokens. Si faltan, se rellenan con 0, si sobran se\n",
    "#  limitará hasta n-2 (inicio con 101 y fin con 102) unidades\n",
    "\n",
    "padded = np.array(ids[:n]+[0]*(n-len(ids)))\n",
    "attention_mask = np.where(padded != 0 ,1, 0)\n",
    "print(ids)\n",
    "print(padded)\n",
    "print(attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
